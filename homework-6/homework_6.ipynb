{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### В качестве dataset’а берем Iris, оставив 2 класса: Iris Versicolor, Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data[data.target > 0]\n",
    "y = data.target[data.target > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Т.к. остались классы 1 и 2, преобразую их в значений 0 и 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Производная от Loss-функции.\n",
    "Функция имеет следующий вид:\n",
    "$$J = -\\sum_{i=1}^{N} y_i\\log (h_\\theta(x_i)) + (1 - y_i)\\log(1 - h_\\theta(x_i))$$\n",
    "\n",
    "Возьмем от нее производную:\n",
    "$$(y_i\\log (h_\\theta(x_i)) + (1 - y_i)\\log(1 - h_\\theta(x_i)))'$$\n",
    "Производная от суммы равна сумме производных:\n",
    "$$((y_i\\log (h_\\theta(x_i)))' + ((1 - y_i)\\log(1 - h_\\theta(x_i)))'$$\n",
    "Раскрываем первую скобку, считая производную от произведения по формуле (x*y)' = x*y' + x'*y:\n",
    "$$y_i(log (h_\\theta(x_i))' + (y_i)'\\log (h_\\theta(x_i) + ((1 - y_i)\\log(1 - h_\\theta(x_i)))'$$\n",
    "Производная от y равна 1, производная от ln(x) равна $\\frac{1}{x}$:\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + log (h_\\theta(x_i)) + ((1 - y_i)\\log(1 - h_\\theta(x_i)))'$$\n",
    "Раскрываем вторую скобку ровно по тем же правилам:\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + log (h_\\theta(x_i)) + (1 - y_i)\\log(1 - h_\\theta(x_i))' + (1 - y_i)'\\log(1 - h_\\theta(x_i))$$\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + log (h_\\theta(x_i)) + \\frac{1 - y_i}{1 - h_\\theta(x_i)} + (1' - y_i')\\log(1 - h_\\theta(x_i))$$\n",
    "Производная от 1 равна нулю:\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + log (h_\\theta(x_i)) + \\frac{1 - y_i}{1 - h_\\theta(x_i)} - \\log(1 - h_\\theta(x_i))$$\n",
    "Переупорядочиваем:\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + \\frac{1 - y_i}{1 - h_\\theta(x_i)} + log (h_\\theta(x_i)) - \\log(1 - h_\\theta(x_i))$$\n",
    "Все это суммируем и берем со знаком минус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClassifier:\n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.1\n",
    "        self.epochs = 2\n",
    "        self.params = []\n",
    "    \n",
    "    def sigmoid(self, y_hat):\n",
    "        return 1 / (1 + np.exp(-y_hat))\n",
    "    \n",
    "    def loss_prime(self, y_true, y_pred):\n",
    "        return (y_true / y_pred) + ((1 - y_true) / (1 - y_pred)) + np.log(y_pred) - np.log(1 - y_pred)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.params = np.random.normal(size=(5,))\n",
    "        for _ in range(self.epochs):\n",
    "            print('--------------------------------------------')\n",
    "            print(self.params)\n",
    "            y_hat = self.params[0] + self.params[1] * X[:, 0] + self.params[2] * X[:, 1] + self.params[3] * X[:, 2] + self.params[4] * X[:, 3]\n",
    "            print(y_hat)\n",
    "            y_pred = self.sigmoid(y_hat)\n",
    "            print(y_pred)\n",
    "            \n",
    "            dl = self.loss_prime(y, y_pred)\n",
    "            print(dl)\n",
    "\n",
    "            self.params[0] -= self.learning_rate * -np.sum(dl)\n",
    "            self.params[1] -= self.learning_rate * -np.sum(dl * X[:, 0])\n",
    "            self.params[2] -= self.learning_rate * -np.sum(dl * X[:, 1])\n",
    "            self.params[3] -= self.learning_rate * -np.sum(dl * X[:, 2])\n",
    "            self.params[4] -= self.learning_rate * -np.sum(dl * X[:, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "[ 0.47597001  0.26733817  1.63044275  0.64271295 -1.07249112]\n",
      "[ 0.24438282  1.64477609  0.73685979  0.87686313 -1.22871486  1.41204963\n",
      "  1.30530551  1.20674363  1.09463744  1.54415537 -0.33860978  1.84248828\n",
      " -0.29584221 -3.58552929 -0.43735484 -2.23045829 -2.05166135  1.31487643\n",
      "  2.53332903 -1.65220042 -0.71707688  2.13554142  1.7576002  -1.22555198\n",
      "  0.75010031 -0.60745453 -0.72489276  1.28630853  0.43566781 -0.09255222\n",
      " -2.5649095   1.48694236 -0.33544691 -1.81132136  2.20838561 -2.08610632\n",
      "  2.15653468 -1.61501699  1.71354542  1.21902876 -1.39514495 -0.09371882\n",
      " -0.65551525 -0.69332602 -2.22876582  0.9914423   1.36368511  1.73246067\n",
      "  6.02801774  1.01341896  0.80629987 -1.60869125  1.60035493  1.28512278\n",
      "  3.16619545  0.89100476  0.99301415  1.15903898  1.14046643  0.47562801\n",
      "  1.70434144  1.06347596 -0.77811263  0.52234982  1.45945048  0.51749447\n",
      "  3.25671937  1.7782705  -0.18017875  0.69768009]\n",
      "[0.56079344 0.83818377 0.6763088  0.70617176 0.22640643 0.80408902\n",
      " 0.78672654 0.76972227 0.74925398 0.82406798 0.41614722 0.86324273\n",
      " 0.4265742  0.02697421 0.39237144 0.09704847 0.11388462 0.78832801\n",
      " 0.92644553 0.16081178 0.328037   0.89430993 0.85290885 0.22696088\n",
      " 0.67920056 0.35264007 0.32631647 0.78352171 0.60722627 0.47687845\n",
      " 0.07143122 0.81561889 0.4169159  0.1404785  0.90100002 0.11045457\n",
      " 0.89627784 0.16589323 0.84729558 0.77189258 0.19858766 0.47658743\n",
      " 0.34174777 0.33329359 0.09719689 0.72937271 0.79635797 0.8497269\n",
      " 0.99759553 0.73368871 0.69132047 0.1667704  0.83206799 0.78332052\n",
      " 0.95954215 0.70909748 0.72968286 0.76115805 0.75776527 0.61671496\n",
      " 0.8461009  0.74335425 0.3147268  0.62769707 0.81144861 0.6265617\n",
      " 0.96291381 0.85548318 0.45507678 0.66767322]\n",
      "[ 2.02757058  2.83783187  3.82622396  2.29294921  0.0639536   2.65569302\n",
      "  5.99412139  2.50591354  2.42929836  7.22816905  2.06438598  9.15471361\n",
      "  1.44806259 -2.5578073   2.11125065 -1.12297913  6.72915566  2.58338393\n",
      " 16.12869586 -0.46057264  0.77110029  3.25372201  8.55610566  0.06804361\n",
      "  2.22241948  2.22829806  2.33961693  5.90570949  2.98166295  2.0044182\n",
      " -1.48798335  2.71300518  2.06311837 -0.64788332  3.3182635   6.96738999\n",
      "  3.27226013 -0.4161297   2.8937711   5.60292831 -0.14734784  2.00453208\n",
      "  0.86365924  2.30703169 -1.1211046   2.36248345  6.27426278  8.38701143\n",
      "  7.030428    4.76842314  2.25280702  4.38757681  2.8021798   5.90023456\n",
      " 27.88327515  2.30124811  2.36347254  2.47282658  5.26869355  2.09712264\n",
      "  2.88623357  2.40872959  0.68115937  3.20833457  2.69181441  3.19531302\n",
      "  4.29523391  8.69788006  1.65494216  3.70676648]\n",
      "--------------------------------------------\n",
      "[25.1194774   7.22212368 15.23007595  3.46268809 -0.87975176]\n",
      "[ 3.80388960e+01  5.37510513e+01  1.71833489e+01  2.99793768e+01\n",
      " -6.36335405e+00  4.55199168e+01  2.67345765e+01  4.46479357e+01\n",
      "  3.17148736e+01  3.84172634e+01  2.19738381e+01  3.83050964e+01\n",
      "  4.33454723e+00 -3.12543931e+01  1.94882410e+01 -1.04346338e+01\n",
      " -6.17956974e+00  5.23017651e+01  4.52239656e+01 -8.95263924e+00\n",
      "  1.06962033e+01  5.36655423e+01  2.55515041e+01 -1.04773605e+00\n",
      "  4.44490248e+01  1.79971434e+01  1.47954164e+01  2.24377010e+01\n",
      "  1.55606433e+01  3.67733808e+01 -6.06802703e+00  4.59437858e+01\n",
      "  2.72894561e+01 -9.22733187e+00  5.23181536e+01  3.74337738e+00\n",
      "  5.39923978e+01 -3.22503014e-01  5.07233679e+01  2.51124951e+01\n",
      " -6.15596435e+00  2.74877428e+01  4.55166417e+00  1.63568695e+01\n",
      " -1.79853993e+01  3.14462314e+01  2.84936920e+01  3.10223490e+01\n",
      "  9.24817012e+01  2.06597686e+01  4.41567638e+01  1.03087330e+01\n",
      "  3.81250024e+01  2.63713357e+01  5.03655265e+01  3.65113998e+01\n",
      "  4.07136768e+01  4.05244797e+01  2.29901388e+01  1.63053174e+01\n",
      "  5.15765321e+01  3.01352279e+01  4.33908413e-03  1.71645320e+01\n",
      "  4.60626274e+01  1.93996795e+01  5.69685292e+01  3.55168869e+01\n",
      "  5.46670510e+00  1.25232326e+01]\n",
      "[1.00000000e+00 1.00000000e+00 9.99999966e-01 1.00000000e+00\n",
      " 1.72061044e-03 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 9.87061784e-01 2.66925170e-14 9.99999997e-01 2.93956689e-05\n",
      " 2.06703737e-03 1.00000000e+00 1.00000000e+00 1.29378464e-04\n",
      " 9.99977370e-01 1.00000000e+00 1.00000000e+00 2.59660078e-01\n",
      " 1.00000000e+00 9.99999985e-01 9.99999625e-01 1.00000000e+00\n",
      " 9.99999825e-01 1.00000000e+00 2.31038734e-03 1.00000000e+00\n",
      " 1.00000000e+00 9.83055397e-05 1.00000000e+00 9.76873486e-01\n",
      " 1.00000000e+00 4.20065866e-01 1.00000000e+00 1.00000000e+00\n",
      " 2.11630657e-03 1.00000000e+00 9.89560500e-01 9.99999921e-01\n",
      " 1.54539795e-08 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 9.99999999e-01 1.00000000e+00 9.99966660e-01\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.99999917e-01\n",
      " 1.00000000e+00 1.00000000e+00 5.01084769e-01 9.99999965e-01\n",
      " 1.00000000e+00 9.99999996e-01 1.00000000e+00 1.00000000e+00\n",
      " 9.95792646e-01 9.99996359e-01]\n",
      "[            nan             nan  2.90157547e+07  3.09798682e+01\n",
      " -5.36163048e+00             nan  4.08008663e+11             nan\n",
      "  3.27129200e+01             inf  2.29738379e+01             inf\n",
      "  8.16249560e+01 -3.02543931e+01  2.04882411e+01 -9.43460442e+00\n",
      "  4.77604621e+02             nan             inf -7.95250984e+00\n",
      "  4.41994669e+04             nan  1.24988888e+11  3.02994837e-01\n",
      "             nan  1.89971435e+01  1.57954168e+01  5.55354100e+09\n",
      "  5.72667812e+06             nan -5.06571129e+00             nan\n",
      "  2.82894926e+01 -8.22723355e+00             nan  4.76705140e+00\n",
      "             nan  1.40183073e+00             nan  8.05782618e+10\n",
      " -5.15384355e+00  2.84878166e+01  1.00341688e+02  1.73568696e+01\n",
      " -1.69853993e+01  3.24485335e+01  2.36906872e+12  2.96289449e+13\n",
      "             nan  9.38476742e+08             nan  1.13087663e+01\n",
      "             nan  2.83744936e+11             inf  3.70436534e+01\n",
      "             nan             nan  9.64917841e+09  1.73053175e+01\n",
      "             nan  3.11355705e+01  2.00868760e+00  2.84748752e+07\n",
      "             nan  2.66179004e+08             nan  2.25179981e+15\n",
      "  2.43145778e+02  2.74657973e+05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myxrome/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n",
      "/home/myxrome/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/home/myxrome/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = GradientClassifier()\n",
    "cls.fit(X_train, y_train)\n",
    "cls.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### На втором шаге постоянно получаю значение сигмоида равное 1, что приводит к делению на 0. Видимо я не правильно вычислил производную для функции потерь или неправильно ее применяю :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
