{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~~В качестве dataset’а берем Iris, оставив 2 класса: Iris Versicolor, Iris Virginica.~~ Исходный набор данных, указанный в условии задачи, к сожалению не давал мне ответа хорошо ли моя логистическая регрессия работает. Моя регрессия, как и sklearn.linear_model.LogisticRegression, после обучения всегда классифицировали одним и тем же классом, давая одинаковое качество 0.6. Мой результат совпадал с решением из коробки, но это ровным счетом ничего не значило. Данные не подходили для линейной модели. Поэтому я модифицировал исходный набор данных, оставив те значения выходных параметров и те искомые значения, которые подходят для работы с линейной моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data[data.target != 2, :2]\n",
    "y = data.target[data.target != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Т.к. остались классы 1 и 2, преобразую их в значений 0 и 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Производная от Loss-функции.\n",
    "~~Функция имеет следующий вид:\n",
    "$$J = -\\sum_{i=1}^{N} y_i\\log (h_\\theta(x_i)) + (1 - y_i)\\log(1 - h_\\theta(x_i))$$\n",
    "Возьмем от нее производную:\n",
    "$$(y_i\\log (h_\\theta(x_i)) + (1 - y_i)\\log(1 - h_\\theta(x_i)))'$$\n",
    "Производная от суммы равна сумме производных:\n",
    "$$((y_i\\log (h_\\theta(x_i)))' + ((1 - y_i)\\log(1 - h_\\theta(x_i)))'$$\n",
    "Раскрываем первую скобку, считая производную от произведения по формуле (x*y)' = x*y' + x'*y:\n",
    "$$y_i(log (h_\\theta(x_i))' + (y_i)'\\log (h_\\theta(x_i) + ((1 - y_i)\\log(1 - h_\\theta(x_i)))'$$\n",
    "Производная от y равна 1, производная от ln(x) равна $\\frac{1}{x}$:\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + log (h_\\theta(x_i)) + ((1 - y_i)\\log(1 - h_\\theta(x_i)))'$$\n",
    "Раскрываем вторую скобку ровно по тем же правилам:\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + log (h_\\theta(x_i)) + (1 - y_i)\\log(1 - h_\\theta(x_i))' + (1 - y_i)'\\log(1 - h_\\theta(x_i))$$\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + log (h_\\theta(x_i)) + \\frac{1 - y_i}{1 - h_\\theta(x_i)} + (1' - y_i')\\log(1 - h_\\theta(x_i))$$\n",
    "Производная от 1 равна нулю:\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + log (h_\\theta(x_i)) + \\frac{1 - y_i}{1 - h_\\theta(x_i)} - \\log(1 - h_\\theta(x_i))$$\n",
    "Переупорядочиваем:\n",
    "$$\\frac{y_i}{h_\\theta(x_i)} + \\frac{1 - y_i}{1 - h_\\theta(x_i)} + log (h_\\theta(x_i)) - \\log(1 - h_\\theta(x_i))$$\n",
    "Все это суммируем и берем со знаком минус.~~\n",
    "\n",
    "#### На самом деле все не так!\n",
    "Я понял, что я неправильно дифференцировал, но что именно я сделал не так, понять у меня не получилось. Судя по всему я ошибся когда брал производную от произведения и от логарифма. Производная от функции sigmoid все же была нужна, после раскрытия логарифма. Так или иначе градиент чудесным образом оказался равен градиенту линейной регрессии и это значительно облегчает задачу! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClassifier:\n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.01, epochs = 100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.params = []\n",
    "    \n",
    "    def sigmoid(self, y_hat):\n",
    "        return 1 / (1 + np.exp(-y_hat))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.params = np.random.normal(size=(3,))\n",
    "        for _ in range(self.epochs):\n",
    "            y_hat = self.params[0] + self.params[1] * X[:, 0] + self.params[2] * X[:, 1] #+ self.params[3] * X[:, 2] + self.params[4] * X[:, 3]\n",
    "            y_pred = self.sigmoid(y_hat)\n",
    "\n",
    "            self.params[0] -= self.learning_rate * np.sum(y_pred - y) / len(y_pred)\n",
    "            self.params[1] -= self.learning_rate * np.sum((y_pred - y) * X[:, 0]) / len(y_pred)\n",
    "            self.params[2] -= self.learning_rate * np.sum((y_pred - y) * X[:, 1]) / len(y_pred)\n",
    "#            self.params[3] -= self.learning_rate * np.sum((y_pred - y) * X[:, 2]) / len(y_pred)\n",
    "#            self.params[4] -= self.learning_rate * np.sum((y_pred - y) * X[:, 3]) / len(y_pred)\n",
    "\n",
    "    def intercept_(self):\n",
    "        return [params[0]]\n",
    "    \n",
    "    def coef_(self):\n",
    "        return [params[1:]]\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = self.params[0] + self.params[1] * X[:, 0] + self.params[2] * X[:, 1] #+ self.params[3] * X[:, 2] + self.params[4] * X[:, 3]\n",
    "        y_pred = self.sigmoid(y_hat)\n",
    "        return (y_pred > 0.5) * 1\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        y_predict = self.predict(X)\n",
    "        return np.sum(y_predict == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4551371089134761] [ 1.07654745 -1.71436838]\n",
      "[1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1]\n",
      "[1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0]\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "cls = GradientClassifier()\n",
    "cls.fit(X_train, y_train)\n",
    "y_pred = cls.predict(X_test)\n",
    "print('{} {}'.format([cls.params[0]], cls.params[1:]))\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(cls.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.30257083] [[ 1.95425259 -3.29682321]]\n",
      "[1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1]\n",
      "[1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0]\n",
      "0.9666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myxrome/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print('{} {}'.format(lr.intercept_, lr.coef_))\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мои коэффициенты отличаются от коэффициентов sklearn.linear_model.LogisticRegression, но имеют одинаковое направление. Возможно, нужно поменять гиперпараметры обучения: шаг обучения и количество итераций. Возможно решение sklearn использует более продвинутый алгоритм, а мое решение не в состоянии найти тот самый минимум и ходит очень рядом с ним. Тем не менее, результаты предсказаний одинаковы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NesterovClassifier:\n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    def __init__(self, ips = 0.975, learning_rate = 0.025, epochs = 100):\n",
    "        self.ips = ips\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.params = []\n",
    "    \n",
    "    def sigmoid(self, y_hat):\n",
    "        return 1 / (1 + np.exp(-y_hat))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.params = np.random.normal(size=(3,))\n",
    "        prev_delta = np.zeros(shape=(3,))\n",
    "        for _ in range(self.epochs):\n",
    "            new_params = self.params - prev_delta\n",
    "            y_hat = new_params[0] + new_params[1] * X[:, 0] + new_params[2] * X[:, 1] #+ new_params[3] * X[:, 2] + new_params[4] * X[:, 3]\n",
    "            y_pred = self.sigmoid(y_hat)\n",
    "            \n",
    "            grad = np.zeros(shape=(3,))\n",
    "            grad[0] = np.sum(y_pred - y) / len(y_pred)\n",
    "            grad[1] = np.sum((y_pred - y) * X[:, 0]) / len(y_pred)\n",
    "            grad[2] = np.sum((y_pred - y) * X[:, 1]) / len(y_pred)\n",
    "\n",
    "            prev_delta[0] = self.ips * prev_delta[0] + self.learning_rate * grad[0]\n",
    "            prev_delta[1] = self.ips * prev_delta[1] + self.learning_rate * grad[1]\n",
    "            prev_delta[2] = self.ips * prev_delta[2] + self.learning_rate * grad[2]\n",
    "            \n",
    "            self.params -= prev_delta\n",
    "\n",
    "    def intercept_(self):\n",
    "        return [params[0]]\n",
    "    \n",
    "    def coef_(self):\n",
    "        return [params[1:]]\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = self.params[0] + self.params[1] * X[:, 0] + self.params[2] * X[:, 1] #+ self.params[3] * X[:, 2] + self.params[4] * X[:, 3]\n",
    "        y_pred = self.sigmoid(y_hat)\n",
    "        return (y_pred > 0.5) * 1\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        y_predict = self.predict(X)\n",
    "        return np.sum(y_predict == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.29186380228296] [ 3.94710537 -6.42473769]\n",
      "[1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1]\n",
      "[1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0]\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "cls = NesterovClassifier()\n",
    "cls.fit(X_train, y_train)\n",
    "y_pred = cls.predict(X_test)\n",
    "print('{} {}'.format([cls.params[0]], cls.params[1:]))\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(cls.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Прогон градиентного спуска очень сильно зависит от начального случайного значения и конечное качество может колебаться от 0.1 до 0.96.  Метод Nesterov показывает более стабильное обучение. Качество ниже 0.9 за 10 прогонов у меня ни разу не опустилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSPropClassifier:\n",
    "    \n",
    "    params = []\n",
    "    \n",
    "    def __init__(self, ips = 0.5, sigma = 0.0001, learning_rate = 0.1, epochs = 100):\n",
    "        self.ips = ips\n",
    "        self.sigma = sigma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.params = []\n",
    "    \n",
    "    def sigmoid(self, y_hat):\n",
    "        return 1 / (1 + np.exp(-y_hat))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.params = np.random.normal(size=(3,))\n",
    "        G = np.zeros(shape=(3,))\n",
    "        for _ in range(self.epochs):\n",
    "            y_hat = self.params[0] + self.params[1] * X[:, 0] + self.params[2] * X[:, 1] #+ self.params[3] * X[:, 2] + self.params[4] * X[:, 3]\n",
    "            y_pred = self.sigmoid(y_hat)\n",
    "            \n",
    "            grad = np.zeros(shape=(3,))\n",
    "            grad[0] = np.sum(y_pred - y) / len(y_pred)\n",
    "            grad[1] = np.sum((y_pred - y) * X[:, 0]) / len(y_pred)\n",
    "            grad[2] = np.sum((y_pred - y) * X[:, 1]) / len(y_pred)\n",
    "\n",
    "            G[0] = self.ips * G[0] + (1 - self.ips) * math.pow(grad[0], 2)\n",
    "            G[1] = self.ips * G[1] + (1 - self.ips) * math.pow(grad[1], 2)\n",
    "            G[2] = self.ips * G[2] + (1 - self.ips) * math.pow(grad[2], 2)\n",
    "            \n",
    "            self.params[0] -= (self.learning_rate / math.sqrt(G[0] + self.sigma)) * grad[0]\n",
    "            self.params[1] -= (self.learning_rate / math.sqrt(G[1] + self.sigma)) * grad[1]\n",
    "            self.params[2] -= (self.learning_rate / math.sqrt(G[2] + self.sigma)) * grad[2]\n",
    "            \n",
    "    def intercept_(self):\n",
    "        return [params[0]]\n",
    "    \n",
    "    def coef_(self):\n",
    "        return [params[1:]]\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = self.params[0] + self.params[1] * X[:, 0] + self.params[2] * X[:, 1] #+ self.params[3] * X[:, 2] + self.params[4] * X[:, 3]\n",
    "        y_pred = self.sigmoid(y_hat)\n",
    "        return (y_pred > 0.5) * 1\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        y_predict = self.predict(X)\n",
    "        return np.sum(y_predict == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.7521530844321347] [ 2.0852589  -2.92359122]\n",
      "[1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1]\n",
      "[1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0]\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "cls = RMSPropClassifier()\n",
    "cls.fit(X_train, y_train)\n",
    "y_pred = cls.predict(X_test)\n",
    "print('{} {}'.format([cls.params[0]], cls.params[1:]))\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(cls.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Данный метод очень сильно зависит от входных параметров. Мне удалось подобрать параметры так, чтобы итоговое качество не опускалось ниже 0.9 от обучения к обучению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P.S.: Операции над множествами через элементы с дублированием строчек не очень элегантное решение, но я так делал для наглядности и решил эту наглядность оставить в конечном варианте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
